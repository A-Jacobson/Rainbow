{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.backends import cudnn\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from utils import grid_from_state\n",
    "\n",
    "from atari_wrappers import wrap_deepmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_state(state):\n",
    "    state = np.array(state, dtype='float32').transpose((2, 0, 1)) # change WHC to CWH for pytorch\n",
    "    state /= 255. # rescale 0-1\n",
    "    state = torch.from_numpy(state).unsqueeze(0) # add batch dim\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "- deepmind wrapper keeps track of a frame stack, resizes to 84x84, and converts to grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "env = wrap_deepmind(env, frame_stack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample state\n",
    "- this is what the q-network will see at every step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAB2CAYAAAAtIuNcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcd40f2e780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "plt.imshow(grid_from_state(state), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "this network will approximate the Q function. It takes a state (4 stacked frames) and outputs a q values for each possible action (Pong has 6 possible actions). Q values are estimates of amount of reward (score) we expect to get at the end of the game.\n",
    "![dqn](assets/dqn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    deepmind architecture from \"Human-level control through deep reinforcement learning\"\n",
    "    https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.conv1(state))  # (20, 20, 32)\n",
    "        x = F.relu(self.conv2(x))  # (9, 9, 64)\n",
    "        x = F.relu(self.conv3(x))  # (7, 7, 64)\n",
    "        x = x.view(x.size(0), -1)  # flatten (7*7*64)\n",
    "        x = F.relu(self.fc4(x))  # (512)\n",
    "        q_values = self.fc5(x)  # (num_actions) q value for each action\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory\n",
    "- we want constant time insertion and sampling as this holds 1 million transitions in original paper and must be sampled from on every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    samples are stored as ('state', 'action', 'next_state', 'reward', done)\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.samples = []\n",
    "        self.insert_location = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        sample = (state, action, reward, next_state, done)\n",
    "        if self.insert_location >= len(self.samples):\n",
    "            self.samples.append(sample)\n",
    "        else:\n",
    "            self.samples[self.insert_location] = sample  # assignment is O(1) for lists\n",
    "        # walk insertion point through list\n",
    "        self.insert_location = (self.insert_location + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.samples))\n",
    "        batch = random.sample(self.samples, batch_size)\n",
    "        return self.prepare_batch(batch)\n",
    "\n",
    "    def prepare_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Transposes and pre-processes batch of transitions into batches of torch tensors\n",
    "        \n",
    "        Args:\n",
    "            batch: list of transitions [[s, a, r, s2, done],\n",
    "                                        [s, a, r, s2, done]]\n",
    "\n",
    "        Returns: [s], [a], [r], [s2], [done_mask]\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, done_mask = [], [], [], [], []\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            states.append(process_state(state))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(process_state(next_state))\n",
    "            done_mask.append(1 - done)  # turn True values into zero for mask\n",
    "        states = torch.cat(states)\n",
    "        next_states = torch.cat(next_states)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        done_mask = torch.FloatTensor(done_mask)\n",
    "        return states, actions, rewards, next_states, done_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection Policy\n",
    "with probability epsilon, select a random action. otherwise selects action corresponding to highest predicted Q value argmax(Q(S, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(q_network, state, env, epsilon):\n",
    "    \"\"\"\n",
    "    epsilon greedy policy.\n",
    "    selects action corresponding to highest predicted Q value, otherwise selects\n",
    "    otherwise selects random action with epsilon probability.\n",
    "    Args:\n",
    "        state: current state of the environment (4 stack of image frames)\n",
    "        epsilon: probability of random action (1.0 - 0.0)\n",
    "\n",
    "    Returns:(int) action to perform\n",
    "    \"\"\"\n",
    "    if epsilon > random.random():\n",
    "        return env.action_space.sample()\n",
    "    state = Variable(process_state(state), volatile=True).cuda()\n",
    "    return int(q_network(state).data.max(1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_epsilon(steps_done, exploration_start=0.9,\n",
    "                      exploration_end=0.05,\n",
    "                      last_exploration_frame=100000):\n",
    "    \"\"\"\n",
    "    calculates exponential decay over eps_decay steps\n",
    "    \"\"\"\n",
    "    eps = exploration_end + (exploration_start - exploration_end) * \\\n",
    "        math.exp(-1. * steps_done / last_exploration_frame)\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![q-learning-algorithm](assets/q-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deep q learning (1 episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper params\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "memory_capacity = 10000\n",
    "learning_rate =1e-4\n",
    "\n",
    "memory = ReplayMemory(capacity=10000) # initialize replay memory\n",
    "q_network = DQN(env.action_space.n).cuda() # initialize action-value function Q with random weights\n",
    "optimizer = Adam(q_network.parameters(), lr=learning_rate)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "state = env.reset() # observe initial state\n",
    "\n",
    "current_step = 0\n",
    "\n",
    "while True:\n",
    "    env.render() # so we can watch!\n",
    "    action = select_action(q_network, state, env, calculate_epsilon(current_step)) # select action\n",
    "    next_state, reward, done, info = env.step(action)  # carry out action/observe reward\n",
    "    # store experience s, a, r, s' in replay memory\n",
    "    memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "    # sample random transitions\n",
    "    states, actions, rewards, next_states, done_mask = memory.sample(batch_size)\n",
    "    \n",
    "    # prepare batch\n",
    "    states = Variable(states).cuda()\n",
    "    next_states = Variable(next_states).cuda()\n",
    "    rewards = Variable(rewards).cuda()\n",
    "    done_mask = Variable(done_mask).cuda()\n",
    "\n",
    "    # calculate target\n",
    "    # find next Q values and set Q values for done states to 0\n",
    "    next_q_values = q_network(next_states).max(dim=1)[0].detach() * done_mask\n",
    "    # calculate targets = rewards + (gamma * next_Q_values)\n",
    "    targets = rewards + (gamma * next_q_values)\n",
    "\n",
    "    q_values = q_network(states)[range(len(actions)), actions]  # select only Q values for actions we took\n",
    "    \n",
    "    # train network\n",
    "    loss = criterion(q_values, targets) # smooth l1 loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient clipping\n",
    "    for param in q_network.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    state = next_state # move to next state\n",
    "    current_step += 1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
