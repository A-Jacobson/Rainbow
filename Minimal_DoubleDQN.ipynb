{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import grid_from_state, process_state\n",
    "from tutorial import select_action, calculate_epsilon\n",
    "from models import DQN\n",
    "from memory import ReplayMemory\n",
    "from atari_wrappers import wrap_deepmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q learning\n",
    "- https://arxiv.org/pdf/1509.06461.pdf\n",
    "- Stabilizes training and sometimes improves performance. \n",
    "- ~5 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAB2CAYAAAAtIuNcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd0446121d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "env = wrap_deepmind(env, frame_stack=True)\n",
    "_ = env.reset()\n",
    "action = env.action_space.sample()\n",
    "state, reward, _, _ = env.step(action)  # carry out action/observe reward\n",
    "next_state, _, _, _ = env.step(env.action_space.sample())  # carry out action/observe reward\n",
    "\n",
    "plt.imshow(grid_from_state(state));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use a second DQN to compute `next_q_values` The target network,\n",
    "with parameters θ−, is the same as the online network except\n",
    "that its parameters are copied every τ steps from the online network,\n",
    "so that then θ−t = θt, and kept fixed on all other steps.\n",
    "\n",
    "- it is common to copy params every 1k-10k frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q network and target network are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_network = DQN(env.action_space.n) # initialize action-value function Q with random weights\n",
    "target_network = DQN(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d (4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d (32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc4): Linear(in_features=3136, out_features=512)\n",
       "  (fc5): Linear(in_features=512, out_features=4)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d (4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d (32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc4): Linear(in_features=3136, out_features=512)\n",
       "  (fc5): Linear(in_features=512, out_features=4)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New target calculation process\n",
    "![doubledqn](assets/doubleq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Variable(process_state(state))\n",
    "r = Variable(torch.Tensor([reward]))\n",
    "s2 = Variable(process_state(next_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = q_network(s)[:, action] # q vals for action we took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_actions = q_network(s2).max(dim=1)[1]\n",
    "next_q_values = target_network(s2)[range(len(target_actions)), target_actions]\n",
    "targets = r + (0.99 * next_q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sync process, every 1-10k steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn_params = q_network.state_dict()\n",
    "target_network.load_state_dict(dqn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new training loop (single episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "learning_rate =1e-4\n",
    "capacity=10000\n",
    "sync_interval=1000 # new parameter t from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(capacity) # initialize replay memory\n",
    "q_network = DQN(env.action_space.n).cuda() # initialize action-value function Q with random weights\n",
    "target_network = DQN(env.action_space.n).cuda() # init target network\n",
    "\n",
    "optimizer = Adam(q_network.parameters(), lr=learning_rate)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "state = env.reset() # observe initial state\n",
    "\n",
    "current_step = 0\n",
    "\n",
    "while True:\n",
    "    env.render() # so we can watch!\n",
    "    action = select_action(q_network, state, env, calculate_epsilon(current_step)) # select action\n",
    "    next_state, reward, done, info = env.step(action)  # carry out action/observe reward\n",
    "    # store experience s, a, r, s' in replay memory\n",
    "    memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "    # sample random transitions\n",
    "    states, actions, rewards, next_states, done_mask = memory.sample(batch_size)\n",
    "    \n",
    "    # prepare batch\n",
    "    states = Variable(states).cuda()\n",
    "    next_states = Variable(next_states).cuda()\n",
    "    rewards = Variable(rewards).cuda()\n",
    "    done_mask = Variable(done_mask).cuda()\n",
    "\n",
    "    # calculate target\n",
    "    # find next Q values and set Q values for done states to 0\n",
    "    \n",
    "    ### DOUBLE Q LEARNING\n",
    "    target_actions = q_network(next_states).max(dim=1)[1]\n",
    "    next_q_values = target_network(next_states)[range(len(target_actions)), target_actions].detach() * done_mask\n",
    "    ### END DOUBLE Q LEARNING\n",
    "    \n",
    "    # calculate targets = rewards + (gamma * next_Q_values)\n",
    "    targets = rewards + (gamma * next_q_values)\n",
    "\n",
    "    q_values = q_network(states)[range(len(actions)), actions]  # select only Q values for actions we took\n",
    "    \n",
    "    # train network\n",
    "    loss = criterion(q_values, targets) # smooth l1 loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient clipping to prevent exploding gradient\n",
    "    for param in q_network.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    state = next_state # move to next state\n",
    "    current_step += 1\n",
    "    \n",
    "    ### DDQN sync\n",
    "    if current_step % sync_interval == 0:\n",
    "            dqn_params = q_network.state_dict()\n",
    "            target_network.load_state_dict(dqn_params)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
